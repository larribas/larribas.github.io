<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>backend on Lorenzo Arribas</title>
    <link>https://larribas.me/tags/backend/</link>
    <description>Recent content in backend on Lorenzo Arribas</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://larribas.me/tags/backend/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Paving the way for distributed asynchronous tasks</title>
      <subtitle>In this article, we discuss queues, schedulers, idempotency and optimistic/pessimistic locking, and how we designed our distributed system.</subtitle>
      <link>https://larribas.me/posts/paving-the-way-for-distributed-asynchronous-tasks/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://larribas.me/posts/paving-the-way-for-distributed-asynchronous-tasks/</guid>
      <description>

&lt;figure&gt;
    

    
    &lt;a href=&#34;https://larribas.me/posts/paving-the-way-for-distributed-asynchronous-tasks/images/clock.jpeg&#34;&gt;
    

    &lt;img
        src=&#34;https://larribas.me/posts/paving-the-way-for-distributed-asynchronous-tasks/images/clock.jpeg&#34;
        alt=&#34;Clock with a road in the background&#34;
        
    &gt;
    &lt;/a&gt;

    &lt;figcaption&gt;Source: &lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;http://sarahrussellpottery.com/galleries/clocks/&#34;&gt;http://sarahrussellpottery.com/galleries/clocks/&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;In a &lt;a href=&#34;https://larribas.me/posts/migrating-glovos-dispatching-service-from-a-single-machine-to-a-distributed-system/&#34;&gt;previous article&lt;/a&gt;, we discussed the dispatching system we use at Glovo to find the best courier to deliver each order.&lt;/p&gt;
&lt;p&gt;To recap, we have a service named Jarvis. Every few seconds, Jarvis retrieves the current state of the city and makes a series of decisions based on mathematical models and optimisation algorithms. We can think of it as &lt;strong&gt;&lt;em&gt;N&lt;/em&gt; tasks running periodically, where &lt;em&gt;N&lt;/em&gt; is the number of cities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Up until a few months ago, Jarvis ran on a single machine. Our goal with this project was to make it run in a distributed cluster and reach high availability.&lt;/p&gt;
&lt;p&gt;In this article, we’ll talk about our journey: the different solutions we considered and how we decided to implement it.&lt;/p&gt;
&lt;h1 id=&#34;making-your-wish-list&#34;&gt;Making your wish list&lt;/h1&gt;
&lt;p&gt;One of the main challenges of designing a distributed system is guaranteeing that it will behave correctly under the uncertainty introduced by network latencies and partitions, instance failures and so on.&lt;/p&gt;
&lt;p&gt;In distributed systems, there are two properties that are hard to guarantee at the same time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Liveness&lt;/strong&gt;, or the promise that the system will eventually work as expected.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safety&lt;/strong&gt;, or the promise that the system will not produce the wrong behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://pt.coursera.org/lecture/cloud-computing/1-4-safety-and-liveness-sFeOE&#34;&gt;This video&lt;/a&gt; provides a good explanation for both.&lt;/p&gt;
&lt;p&gt;In our case, we &lt;em&gt;needed&lt;/em&gt; safety. If two workers published conflicting decisions about the same city, it would be extremely difficult to understand the consequences of these decisions and fix them after the fact.&lt;/p&gt;
&lt;p&gt;At the same time, liveness was important. If a worker suffered, say, a hardware failure, we wanted another worker to eventually pick up the task, with no human intervention.&lt;/p&gt;
&lt;p&gt;So we decided to evaluate different solutions in terms of these properties. On top of that, we added other nice-to-haves, like simplicity, ease to onboard people, or ease to mitigate and debug if something goes very wrong.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lesson one: Before you start evaluating different potential implementations, make sure you have a good framework to compare them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;queues-and-workers&#34;&gt;Queues and Workers&lt;/h1&gt;
&lt;p&gt;A very common pattern is to have a series of workers subscribed to a queue. Every time there is a task to run, a worker would pick it up and start processing it.&lt;/p&gt;
&lt;p&gt;This is a very common architecture, and there are many technologies that would make the implementation somewhat trivial, such as &lt;a href=&#34;https://aws.amazon.com/sqs/&#34;&gt;AWS SQS&lt;/a&gt;, &lt;a href=&#34;https://www.rabbitmq.com/&#34;&gt;RabbitMQ&lt;/a&gt;, &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Kafka&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That said, since we are dealing with a periodic task, we would still need to have some kind of process that schedules new jobs, tries not to overflow the queue, and is aware of domain events like a city being created or disabled. On top of this, the workers would still need to ensure they don’t make conflicting decisions for the same city at the same time, either by being idempotent, or having some kind of locking mechanism.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lesson two: If a solution for a complex problem looks too easy, be suspicious. Is it that great, or is it hiding the complexity under the rug?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Note: If you ever go with a queue system to distribute a task, keep in mind that &lt;a href=&#34;https://bravenewgeek.com/you-cannot-have-exactly-once-delivery/&#34;&gt;queues cannot guarantee each message is delivered exactly once&lt;/a&gt;. Your options are either at-most-once delivery (if you’re okay with missing some executions), and at-least-once delivery, which would require your task to be idempotent.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;do-developers-dream-of-idempotent-tasks&#34;&gt;Do developers dream of idempotent tasks?&lt;/h1&gt;
&lt;p&gt;Generally speaking, when you want to distribute a task (or a message subscriber), your ideal scenario is making the behavior idempotent which means that it doesn’t really matter whether it runs once or ten times: the outcome will be the same. Then, issues like race conditions become irrelevant and distributing the tasks is somewhat trivial.&lt;/p&gt;
&lt;p&gt;For a while, we were toying with the idea, but pretty soon we realised it would have required a radical redesign of our service.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lesson three: When the ideal solution is not practical, it’s not a solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;trusting-the-scheduler&#34;&gt;Trusting the scheduler&lt;/h1&gt;
&lt;p&gt;Another popular option is to rely on a task scheduling system to ensure tasks are run periodically and with specific constraints. This is the job of technologies like &lt;a href=&#34;https://sidekiq.org/&#34;&gt;Sidekiq&lt;/a&gt; in the Ruby community, or &lt;a href=&#34;http://www.quartz-scheduler.org/&#34;&gt;Quartz&lt;/a&gt; in Java’s.&lt;/p&gt;
&lt;p&gt;These tools were designed to solve a lot of the problems we had but, in order to ensure high availability, we would need to deploy and maintain a cluster of schedulers that worked in consensus. Otherwise, they would become a single point of failure for our own system.&lt;/p&gt;
&lt;p&gt;Our dispatching system was complex enough as it was, and the simpler the solution we managed to find, the easier it would be to maintain it in the future.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lesson four: Some problems are complex, but so is maintaining 3rd-party services. Think about which of the two would minimise the error surface of your application.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;shared-resources-and-locks&#34;&gt;Shared resources and locks&lt;/h1&gt;
&lt;p&gt;Once we discarded the idea of having an external system telling our workers what to do and when to do it, we started thinking about alternative ways to coordinate them.&lt;/p&gt;
&lt;p&gt;The conclusion we came to was that we needed a central resource (say, a database table) that contained locks for all existing cities. When a worker found a lock that was free, it would grab it and execute the task.&lt;/p&gt;
&lt;p&gt;From the very beginning, our team was divided between &lt;a href=&#34;https://en.wikipedia.org/wiki/Record_locking&#34;&gt;pessimistic&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Optimistic_concurrency_control&#34;&gt;optimistic locking&lt;/a&gt;. The first technique involves creating a lock that, while held by a worker, blocks any other worker from acquiring the same resource. This would be safe and avoid situations like two couriers being assigned to the same order (scary stuff!).&lt;/p&gt;
&lt;p&gt;That said, the other half of our team was even more scared of the following situation:&lt;/p&gt;


&lt;figure&gt;
    

    
    &lt;a href=&#34;https://larribas.me/posts/paving-the-way-for-distributed-asynchronous-tasks/images/fencing.png&#34;&gt;
    

    &lt;img
        src=&#34;https://larribas.me/posts/paving-the-way-for-distributed-asynchronous-tasks/images/fencing.png&#34;
        alt=&#34;Without any fencing strategy, if a worker took longer than its timeout to complete, it would not be able to commit.&#34;
        
    &gt;
    &lt;/a&gt;

    &lt;figcaption&gt;Without any fencing strategy, if a worker took longer than its timeout to complete, it would not be able to commit.&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;What would happen if a machine hung up or died? Sure, we could have a reasonable timeout for the lock and have another worker pick up the task after a while, but even that wouldn’t protect us completely from livelocks.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After some interesting &lt;!-- raw HTML omitted --&gt;bribes&lt;!-- raw HTML omitted --&gt; debates, we decided to see the glass half full and go with an optimistic locking strategy, where two tasks may run at the same time, but ultimately only one will be able to modify the database, via a &lt;a href=&#34;https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html&#34;&gt;fencing strategy&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;I’d like to thank Andre Lopes for his review and his constant search for new edge cases and guarantees :P&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the next article, we’ll get a bit more practical and discuss the implementation of an optimistic locking mechanism using MySQL 5.6 and Java. Stay tuned!&lt;/p&gt;
</description>
      
      <category>distributed-systems</category>
      
      <category>high-availability</category>
      
      <category>backend</category>
      
      <category>web-services</category>
      
    </item>
    
    <item>
      <title>Meaningful web service /health checks</title>
      <subtitle>Health checks are an important part of the lifecycle of our systems. Here are 10 best practices to make them richer and more useful.</subtitle>
      <link>https://larribas.me/posts/meaningful-web-service-health-checks/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://larribas.me/posts/meaningful-web-service-health-checks/</guid>
      <description>&lt;p&gt;About 10 years ago, I deployed my first web service. It was a nice, silly PHP application to store game cheat sheets. Interestingly, what made me &lt;em&gt;really&lt;/em&gt; proud about it was the fact that I was able to release new versions with a single command, with a weird mix of git hooks and rsync-powered bash scripts.&lt;/p&gt;
&lt;p&gt;When I think about the massive transformation that our field has undergone in the last few years in terms of continuous delivery and service orchestration, I always come back to that memory, and I can&amp;rsquo;t help laughing a bit.&lt;/p&gt;
&lt;p&gt;Cloud platforms such as &lt;em&gt;AWS&lt;/em&gt;, &lt;em&gt;Heroku&lt;/em&gt;, &lt;em&gt;Azure&lt;/em&gt; or &lt;em&gt;Kubernetes&lt;/em&gt; have enabled us to use deployment strategies, such as canary releases, staged rollouts, or blue-green deployments, regardless of whether we&amp;rsquo;re deploying a side project or a critical enterprise service.&lt;/p&gt;
&lt;p&gt;All of these strategies have but one goal: to &lt;strong&gt;minimise client-facing downtime&lt;/strong&gt;. Which brings me to an important (yet somehow easily forgotten) topic: Health checks.&lt;/p&gt;


&lt;figure&gt;
    

    
    &lt;a href=&#34;https://larribas.me/posts/meaningful-web-service-health-checks/images/ok_xray_scan.jpeg&#34;&gt;
    

    &lt;img
        src=&#34;https://larribas.me/posts/meaningful-web-service-health-checks/images/ok_xray_scan.jpeg&#34;
        alt=&#34;X-ray scan of a hand saying &amp;#39;ok&amp;#39;&#34;
        
    &gt;
    &lt;/a&gt;

    
&lt;/figure&gt;


&lt;p&gt;A health check is a way for a service to report whether it&amp;rsquo;s running properly or not. Web services usually expose this via a &lt;code&gt;/health&lt;/code&gt; HTTP endpoint. Then orchestration components, such as load balancers or service discovery systems, poll that endpoint to monitor the health of a fleet of services and make some key decisions, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is a new version of the service ready to receive requests?&lt;/li&gt;
&lt;li&gt;Should we roll back a deployment?&lt;/li&gt;
&lt;li&gt;Should we restart an instance?&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;anemic-healthchecks&#34;&gt;Anemic health checks&lt;/h1&gt;
&lt;p&gt;In my short experience (I&amp;rsquo;ve been in the industry for about 6 years), I&amp;rsquo;ve seen a bunch of health checks for different services and I&amp;rsquo;ve realised that most of them are pretty basic. They attempt to establish a connection to their downstream dependencies (select something from the database, ping Redis…) and report that they&amp;rsquo;re okay as soon as they:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can process a request to the &lt;code&gt;/health&lt;/code&gt; endpoint (that must mean our application is fully loaded);&lt;/li&gt;
&lt;li&gt;Can connect to their dependencies.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After that, we&amp;rsquo;re basically good to go.&lt;/p&gt;
&lt;p&gt;So when I needed to write my own health check for a new service, I copied that pattern.&lt;/p&gt;
&lt;p&gt;And then we had an outage. We had deployed a new version with a bug and my engineering manager asked me a very basic question: &lt;strong&gt;Is our health check giving us enough information to prevent this?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That made me reflect and realise that our health checks were anemic. We speak of &lt;a href=&#34;https://en.wikipedia.org/wiki/Anemic_domain_model&#34;&gt;anemic domain models&lt;/a&gt; when a domain is so silly that it doesn&amp;rsquo;t contain any business logic. It&amp;rsquo;s just there, being dull and irrelevant. And so were our health checks.&lt;/p&gt;
&lt;h1 id=&#34;10-rules-for-meaningful-health-checks&#34;&gt;10 rules for meaningful health checks&lt;/h1&gt;
&lt;p&gt;I immediately turned to Google, Stack Overflow &amp;amp; co. looking for best practices for health checks. I couldn’t find any! No philosophical discussions on health checks, no epic rants about how we’re doing everything wrong…&lt;/p&gt;
&lt;p&gt;Maybe I had lost my googling mojo or maybe nobody cared about health checks, but I do! And if you do too, here’s a good list of things to keep in mind when you’re implementing a health check for your service.&lt;/p&gt;
&lt;h2 id=&#34;rule-1-take-your-business-use-cases-into-account&#34;&gt;Rule #1. Take your business use cases into account&lt;/h2&gt;
&lt;p&gt;Being healthy can mean different things in different contexts. For typical web services exposing an HTTP API, it might be enough to consider the ratio of internal server errors. For other services, such as tasks that need to run periodically or subscribers that need to consume events, a healthy state might mean something completely different.&lt;/p&gt;
&lt;p&gt;When asked the question: &lt;em&gt;when am I operating healthily?&lt;/em&gt;, think twice about the use cases your service is supposed to fulfil.&lt;/p&gt;
&lt;h2 id=&#34;rule-2-check-your-downstream-dependencies&#34;&gt;Rule #2. Check your downstream dependencies&lt;/h2&gt;
&lt;p&gt;A health check shouldn’t just rely on its downstream dependencies, but they’re definitely an important part of the equation.&lt;/p&gt;
&lt;p&gt;Typically, you’ll want to answer these questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can I grab a connection from my connection pool?&lt;/li&gt;
&lt;li&gt;Can I request something simple from the database?&lt;/li&gt;
&lt;li&gt;Does the request finish within an acceptable time?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rule-3-return-machine-readable-data&#34;&gt;Rule #3. Return machine-readable data&lt;/h2&gt;
&lt;p&gt;Health checks will be mainly used by machines for a wide range of scenarios (visualisation, decision-making, load balancing, alerting…).&lt;/p&gt;
&lt;p&gt;Your health checks should return data in a machine-readable format (there’s a &lt;a href=&#34;https://inadarei.github.io/rfc-healthcheck/&#34;&gt;nice RFC proposing a standard&lt;/a&gt;) that looks the same for all the services of your company.&lt;/p&gt;
&lt;p&gt;For bonus points, try to be transparent about the checks you perform, which ones failed and why. As your deployment and routing strategies get more sophisticated, this information will become invaluable.&lt;/p&gt;
&lt;h2 id=&#34;rule-4-report-health-as-a-spectrum&#34;&gt;Rule #4. Report health as a spectrum&lt;/h2&gt;
&lt;p&gt;People’s health is not binary. They are not either completely healthy or dead. Servers aren’t either.&lt;/p&gt;
&lt;p&gt;There are several reasons why we might want to react to an unhealthy status. We might want to roll back a release, restart the service, reduce traffic, page our on-call engineer. This will highly depend on our ability to distinguish between different shades of grey.&lt;/p&gt;
&lt;p&gt;Can you imagine getting back a medical report with no more information than OK or KO?&lt;/p&gt;
&lt;h2 id=&#34;rule-5-consider-different-checks-readiness-liveness&#34;&gt;Rule #5. Consider different checks (readiness, liveness…)&lt;/h2&gt;
&lt;p&gt;Orchestration platforms like &lt;em&gt;Kubernetes&lt;/em&gt; make a distinction between a liveness check and a readiness check (although, in &lt;em&gt;Kubernetes&lt;/em&gt; jargon, they call them probes).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;readiness check&lt;/strong&gt; answers the question: &lt;em&gt;Can I start processing work?&lt;/em&gt; Plus, it might check things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can I establish a connection to the database?&lt;/li&gt;
&lt;li&gt;Are all important caches warmed up?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A &lt;strong&gt;liveness check&lt;/strong&gt; answers the question: &lt;em&gt;Should I keep running?&lt;/em&gt; It might depend on things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is my error rate acceptable?&lt;/li&gt;
&lt;li&gt;Am I running out of memory? Might I have a slow memory leak?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rule-6-dont-confuse-overall-health-with-individual-health&#34;&gt;Rule #6. Don’t confuse overall health with individual health&lt;/h2&gt;
&lt;p&gt;A health check is concerned with the health of a particular instance of your server, so we don’t want to report health based on aggregated metrics like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The overall error rate of your cluster;&lt;/li&gt;
&lt;li&gt;The number of customer sign-ups.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact, the whole point is that you spot the bad apples from the good ones, and replace them without affecting your end customer at all.&lt;/p&gt;
&lt;h2 id=&#34;rule-7-dont-expose-the-health-endpoint-publicly&#34;&gt;Rule #7. Don’t expose the health endpoint publicly&lt;/h2&gt;
&lt;p&gt;Health endpoints are supposed to contain debug-level information. They leak very important details about their internal implementation, what your service uses and what it doesn’t use.&lt;/p&gt;
&lt;p&gt;You can be transparent with the community and keep a &lt;a href=&#34;https://www.githubstatus.com/&#34;&gt;nice status page&lt;/a&gt;, but keep the endpoint itself safe behind closed doors.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Treat the privacy of your services as you would treat the privacy of your customers. They go hand in hand.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;rule-8-delegate-to-smaller-subcomponent-checks&#34;&gt;Rule #8. Delegate to smaller subcomponent checks&lt;/h2&gt;
&lt;p&gt;Some services are fairly small. Some are big monoliths. In the first case, you might not be worried if your health endpoint is tightly coupled with some other parts of your system. If you’re checking the health of a big monolith, however, your concerns might be a bit different.&lt;/p&gt;
&lt;p&gt;If it makes sense for you, consider providing a way for other components to implement their own health checks in a way that the main health endpoint can invoke them without being coupled to its internal representation.&lt;/p&gt;
&lt;p&gt;Be careful, however, and think about the ripple effect that you want each component to have over your monolith. Should you report an unhealthy instance just because one component is in a failing state? Should you keep your report waiting if a component takes too long to process theirs?&lt;/p&gt;
&lt;h2 id=&#34;rule-9-health-checks-should-be-efficient&#34;&gt;Rule #9. Health checks should be efficient&lt;/h2&gt;
&lt;p&gt;If you have a high-volume service, an endpoint that gets queried a dozen times per minute might look like a drop in the ocean. However, keep in mind that the orchestration services calling your health endpoint also have timeouts and they might decide you’re unhealthy if you take too long to answer. For that reason, it pays to follow some simple patterns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Perform every individual check in parallel and join the results.&lt;/li&gt;
&lt;li&gt;Use timeouts to ensure the latency is within acceptable bounds.&lt;/li&gt;
&lt;li&gt;Even better, perform the checks periodically in the background and keep a centralized, up-to-date status. That way, the health endpoint can return immediately and you will not be limited by its performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rule-10-monitor-the-history-of-your-health-checks&#34;&gt;Rule #10. Monitor the history of your health checks&lt;/h2&gt;
&lt;p&gt;Health checks make for very good time-series data.&lt;/p&gt;
&lt;p&gt;Whenever you generate a health report, send metrics to your observability system. This will enable you to answer questions such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How long does it take for my instances to become ready?&lt;/li&gt;
&lt;li&gt;For how long has my system remained healthy?&lt;/li&gt;
&lt;li&gt;How often do I have partially unhealthy states? What are the causes?&lt;/li&gt;
&lt;li&gt;How many requests is my health check receiving?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which in turn will allow you to prevent issues, identify the root cause of an outage, or optimise some key areas of your application.&lt;/p&gt;
&lt;h1 id=&#34;in-summary&#34;&gt;In summary&lt;/h1&gt;
&lt;p&gt;Orchestration platforms allow us to control the availability and reliability of our services, and reduce the risk of deploying a bad version of our code.&lt;/p&gt;
&lt;p&gt;These deployment strategies rely heavily on our ability to report the health of our instances in an accurate and timely way.&lt;/p&gt;
&lt;p&gt;Our services have a rich business logic, and the health checks we write should reflect that wealth of use cases and follow some best practices to ensure we make the most out of them.&lt;/p&gt;
</description>
      
      <category>dev-ops</category>
      
      <category>backend</category>
      
      <category>web-services</category>
      
      <category>continuous-deployment</category>
      
    </item>
    
    <item>
      <title>Migrating Glovo’s dispatching service from a single machine to a distributed system</title>
      <subtitle>How we re-engineered one of our main web services to make it highly available. Part One.</subtitle>
      <link>https://larribas.me/posts/migrating-glovos-dispatching-service-from-a-single-machine-to-a-distributed-system/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://larribas.me/posts/migrating-glovos-dispatching-service-from-a-single-machine-to-a-distributed-system/</guid>
      <description>&lt;p&gt;At Glovo, we have one vision: Connecting our customers to their cities. We do that by facilitating access to different services (like restaurants, supermarkets, and other stores) through a single mobile application and a fleet of independent couriers.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Anything you want. Delivered in minutes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course, that is easier said than done. Behind the scenes, there are hundreds of factors that affect the experience of our customers and couriers.&lt;/p&gt;
&lt;p&gt;One of the key factors in this equation is &lt;strong&gt;who is the perfect courier to deliver an order&lt;/strong&gt;. This is very important for us because finding the best match means our customers will receive their orders faster, our couriers will deliver more orders every hour (and earn more money), and our partners will be able to receive more orders.&lt;/p&gt;
&lt;p&gt;Answering that question is the main job of our dispatching team. And more specifically: the job of &lt;em&gt;Jarvis&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;meet-jarvis-our-dispatching-service&#34;&gt;Meet Jarvis, our Dispatching Service&lt;/h2&gt;


&lt;figure&gt;
    

    
    &lt;a href=&#34;https://larribas.me/posts/migrating-glovos-dispatching-service-from-a-single-machine-to-a-distributed-system/images/mainframe.jpeg&#34;&gt;
    

    &lt;img
        src=&#34;https://larribas.me/posts/migrating-glovos-dispatching-service-from-a-single-machine-to-a-distributed-system/images/mainframe.jpeg&#34;
        alt=&#34;Busy people around an old mainframe computer&#34;
        
    &gt;
    &lt;/a&gt;

    &lt;figcaption&gt;This is how I feel our team works when we&amp;rsquo;re having a live issue with Jarvis. &lt;!-- raw HTML omitted --&gt;Source&lt;!-- raw HTML omitted --&gt;&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;Jarvis is Glovo&amp;rsquo;s dispatching service. Its goal is to make cities as efficient as possible. I&amp;rsquo;d say Jarvis stands out from other services in three different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Its domain is very mathematical&lt;/strong&gt;. We are running an optimisation problem based on estimations we get from machine learning models (e.g. how much time will it take the courier to reach a point? When will the partner have the order ready?).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Its behavior is different in every city&lt;/strong&gt;. Each city has its own geography, its own demographic distribution, its own traffic… and, most importantly, its own personality. We need to account for those differences; adjusting thresholds, activating different heuristics, and so on.&lt;/li&gt;
&lt;li&gt;During peak hours, our highest-volume cities will have tens of thousands of couriers in different areas and states. At the same time, we will have hundreds of new orders coming in every minute. We need to be able to evaluate every possible combination of orders and couriers, and choose the best match in real-time, so &lt;strong&gt;high performance is critical&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;from-pets-tocattle&#34;&gt;From pets to cattle&lt;/h2&gt;
&lt;p&gt;A few months ago, we had a big problem: Jarvis was one of the most critical pieces in our system. However, it was also one of the most fragile.&lt;/p&gt;
&lt;p&gt;This is how everything worked back then:&lt;/p&gt;


&lt;figure&gt;
    

    
    &lt;a href=&#34;https://larribas.me/posts/migrating-glovos-dispatching-service-from-a-single-machine-to-a-distributed-system/images/diagram.png&#34;&gt;
    

    &lt;img
        src=&#34;https://larribas.me/posts/migrating-glovos-dispatching-service-from-a-single-machine-to-a-distributed-system/images/diagram.png&#34;
        alt=&#34;Diagram of our dispatching system running on a single instance, and how releases and rollbacks affected its availability&#34;
        
    &gt;
    &lt;/a&gt;

    &lt;figcaption&gt;Jarvis ran on one single instance. This instance used the actor concurrency model to make decisions for different cities concurrently. Whenever there was a release (and a rollback), we would have a few minutes of downtime, delaying order delivery.&lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;We used the actor model to run hundreds of concurrent actors. Each actor would control a particular city. Our main problem was that two actors could not be in charge of the same city, as they could potentially make conflicting decisions. This meant we could not distribute the service: we needed to rely on a single physical machine.&lt;/p&gt;
&lt;p&gt;As a result, every time we deployed a new version we would have a few minutes of downtime, increasing our delivery time (one of our most important KPIs). If the version was faulty and we needed to roll back, we would have twice as much downtime.&lt;/p&gt;
&lt;p&gt;Last quarter, we decided to change this and transition from a we-have-a-single-machine-please-cross-your-fingers architecture to a highly available, distributed architecture.&lt;/p&gt;
&lt;p&gt;The task was at the same time daunting and very interesting. We had to make some fundamental changes to our system, but it was absolutely essential to make them gradually, without disrupting the service at all. To put it another way: we needed to change the plane&amp;rsquo;s engine in the middle of the flight.&lt;/p&gt;
&lt;h2 id=&#34;whiteboard-session-no1-requirements&#34;&gt;Whiteboard session No.1: Requirements&lt;/h2&gt;
&lt;p&gt;The first thing we did was gather the whole team and come up with a list of requirements. Roughly speaking:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The service should be distributed. At any time, there should be multiple machines in different availability zones or regions, competing to run the assignment problem for a city.&lt;/li&gt;
&lt;li&gt;It is vital that we ensure the service&amp;rsquo;s liveness: if a machine dies while making decisions for a particular city, it should not prevent other machines from picking up that city in the future.&lt;/li&gt;
&lt;li&gt;The decisions we make must not cause conflicts (e.g. we don&amp;rsquo;t want to assign two orders to a courier at the same time). Therefore, two tasks for the same city may not run concurrently or, if they do, both should not publish the decisions.&lt;/li&gt;
&lt;li&gt;We should strive for simplicity. Jarvis is a critical piece of our system. The more technical complexity we add, and the more services we depend upon, the higher the chance of service unavailability.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;whiteboard-session-no2-andbeyond&#34;&gt;Whiteboard session No.2: (And beyond…)&lt;/h2&gt;
&lt;p&gt;We don&amp;rsquo;t want to make this post overly long and, to tell the truth, we would be really glad to hear from you and how you would approach this problem! So let&amp;rsquo;s leave it here for now.&lt;/p&gt;
&lt;p&gt;In the next article, we&amp;rsquo;ll follow up with our approach, with a couple of surprising plot twists that made us redefine our solution.&lt;/p&gt;
</description>
      
      <category>distributed-systems</category>
      
      <category>web-services</category>
      
      <category>high-availability</category>
      
      <category>backend</category>
      
    </item>
    
  </channel>
</rss>
